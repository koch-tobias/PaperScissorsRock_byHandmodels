{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model engineering - Part: Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Research question**\n",
    "Can transfer learning bring a benefit on the performance of CNN models for Rock, Paper, Scissors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** </br>\n",
    "Code according to: [learnpytorch.io](https://www.learnpytorch.io/05_pytorch_going_modular/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from timeit import default_timer as timer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dir: str, \n",
    "                            test_dir: str, \n",
    "                            transform: transforms.Compose, \n",
    "                            batch_size: int, \n",
    "                            num_workers: int=1\n",
    "                        ):\n",
    "    \n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "   \n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(train_data,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True\n",
    "                                )\n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=num_workers,\n",
    "                                    pin_memory=True\n",
    "                                )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_efficientNet_B0():\n",
    "\n",
    "    # Load best available weights from pretraining on ImageNet\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    \n",
    "    # Load pretrained model with selected weights\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "\n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dir: str, test_dir: str, weights, num_workers: int, batch_size: int):\n",
    "    # Get the transforms used to create our pretrained weights\n",
    "    auto_transforms = weights.transforms()\n",
    "\n",
    "    # Create training and testing DataLoaders as well as get a list of class names\n",
    "    train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                test_dir=test_dir,\n",
    "                                                                                transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n",
    "                                                                                batch_size=batch_size, # set mini-batch size to 32\n",
    "                                                                                num_workers=num_workers) \n",
    "\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to recreate the classifier layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_classifier_layer(model: torch.nn.Module, dropout: int, class_names: list):\n",
    "    # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Set the manual seeds\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Recreate the classifier layer\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "                            torch.nn.Dropout(p=dropout, inplace=True), \n",
    "                            torch.nn.Linear(in_features=1280, \n",
    "                            out_features=len(class_names), # one output unit for each class\n",
    "                            bias=True))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "                    dataloader: torch.utils.data.DataLoader, \n",
    "                    loss_fn: torch.nn.Module, \n",
    "                    optimizer: torch.optim.Optimizer\n",
    "                ) -> Tuple[float, float]:\n",
    "\n",
    "  # Put model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "      # 1. Forward pass\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # 2. Calculate  and accumulate loss\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      train_loss += loss.item() \n",
    "\n",
    "      # 3. Optimizer zero grad\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 4. Loss backward\n",
    "      loss.backward()\n",
    "\n",
    "      # 5. Optimizer step\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate and accumulate accuracy metric across all batches\n",
    "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module\n",
    "              ) -> Tuple[float, float]:\n",
    "\n",
    "  # Put model in eval mode\n",
    "  model.eval() \n",
    "\n",
    "  # Setup test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "      # Loop through DataLoader batches\n",
    "      for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "          # 1. Forward pass\n",
    "          test_pred_logits = model(X)\n",
    "\n",
    "          # 2. Calculate and accumulate loss\n",
    "          loss = loss_fn(test_pred_logits, y)\n",
    "          test_loss += loss.item()\n",
    "\n",
    "          # Calculate and accumulate accuracy\n",
    "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, \n",
    "            train_dataloader: torch.utils.data.DataLoader, \n",
    "            test_dataloader: torch.utils.data.DataLoader, \n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            loss_fn: torch.nn.Module,\n",
    "            epochs: int\n",
    "            ) -> Dict[str, List]:\n",
    "\n",
    "  # Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "                \"train_acc\": [],\n",
    "                \"test_loss\": [],\n",
    "                \"test_acc\": []\n",
    "            }\n",
    "\n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "      train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer\n",
    "                                          )\n",
    "      test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn\n",
    "          )\n",
    "\n",
    "      # Print out what's happening\n",
    "      print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "      )\n",
    "\n",
    "      # Update results dictionary\n",
    "      results[\"train_loss\"].append(train_loss)\n",
    "      results[\"train_acc\"].append(train_acc)\n",
    "      results[\"test_loss\"].append(test_loss)\n",
    "      results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the filled results at the end of the epochs\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "data_path = '../data_original/dataset_1'\n",
    "train_dir = data_path + \"/train\"\n",
    "test_dir = data_path + \"/test\"\n",
    "\n",
    "# Set parameters\n",
    "seed = 42\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "dropout = 0.2\n",
    "num_workers = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Load pretrained model, weights and the transforms\n",
    "model, weights = load_pretrained_efficientNet_B0()\n",
    "\n",
    "# Load data\n",
    "train_dataloader, test_dataloader, class_names = load_data(train_dir=train_dir,\n",
    "                                                                test_dir=test_dir, \n",
    "                                                                weights=weights, \n",
    "                                                                num_workers=num_workers, \n",
    "                                                                batch_size=batch_size\n",
    "                                                            )\n",
    "\n",
    "# Recreate classifier layer\n",
    "model = recreate_classifier_layer(model=model, \n",
    "                                        dropout=dropout, \n",
    "                                        class_names=class_names\n",
    "                                    )\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set the random seeds\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Start the timer\n",
    "start_time = timer()\n",
    "\n",
    "# Setup training and save the results\n",
    "results = train(model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    test_dataloader=test_dataloader,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    epochs=epochs\n",
    "                )\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('envPaperScissorsRock')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "246e1f8a7853b7843a5a4bae4dd15b195eef0a7489bca88a3b586940aa1ce5a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
